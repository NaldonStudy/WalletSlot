{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd4b8b9-2439-4ff6-8269-ac20da1ddb8d",
   "metadata": {},
   "source": [
    "### build_staging_from_merged.py -> 표준화/정제(region 통일, 카테고리 분리, 이름 공백행 제거 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e052fe1-af25-47c2-b522-254862030486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 포털DB: 4,321,425행\n",
      "[OK] 기업DB: 46,841행\n",
      "[OK] 인허가DB: 7,826,824행\n",
      "[OK] 법인DB: 730,602행\n",
      "[OK] 마켓DB: 546,150행\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_19168\\2176795729.py:262: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  staging_cp = staging.applymap(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 저장 완료 → staging_all.utf8.csv / staging_all.csv (총 13,471,842행)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "staging 빌더 스크립트\n",
    "- 입력: ./_merged_csv 폴더 내 각 원본(DB) 통합 CSV들(예: 포털DB_merged.csv 등)\n",
    "- 처리: 소스별 스키마를 표준 스키마로 정규화 + 카테고리 분리/JSON화 + region 표준화\n",
    "- 출력: staging_all.utf8.csv (UTF-8) / staging_all.csv (CP949)\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# =========================================\n",
    "# 경로 설정 (현재 작업 디렉터리 기준 상대경로)\n",
    "# =========================================\n",
    "MERGED_DIR = Path(\"./_merged_csv\")         # *_merged.csv 들이 있는 폴더\n",
    "OUT_PATH_UTF8  = MERGED_DIR / \"staging_all.utf8.csv\"\n",
    "OUT_PATH_CP949 = MERGED_DIR / \"staging_all.csv\"\n",
    "\n",
    "# =========================================\n",
    "# 지역 정규화 사전\n",
    "#  - 다양한 표기(서울특별시/서울시/서울 등)를 단일 키(예: \"서울\")로 통일\n",
    "# =========================================\n",
    "REGION_VARIANTS: Dict[str, List[str]] = {\n",
    "    \"서울\": [\"서울특별시\", \"서울시\", \"서울\"],\n",
    "    \"부산\": [\"부산광역시\", \"부산시\", \"부산\"],\n",
    "    \"대구\": [\"대구광역시\", \"대구시\", \"대구\"],\n",
    "    \"인천\": [\"인천광역시\", \"인천시\", \"인천\"],\n",
    "    \"광주\": [\"광주광역시\", \"광주시\", \"광주\"],\n",
    "    \"대전\": [\"대전광역시\", \"대전시\", \"대전\"],\n",
    "    \"울산\": [\"울산광역시\", \"울산시\", \"울산\"],\n",
    "    \"세종\": [\"세종특별자치시\", \"세종시\", \"세종\"],\n",
    "    \"경기\": [\"경기도\", \"경기\"],\n",
    "    \"강원\": [\"강원특별자치도\", \"강원도\", \"강원\"],\n",
    "    \"충북\": [\"충청북도\", \"충북\"],\n",
    "    \"충남\": [\"충청남도\", \"충남\"],\n",
    "    \"전북\": [\"전라북도\", \"전북\"],\n",
    "    \"전남\": [\"전라남도\", \"전남\"],\n",
    "    \"경북\": [\"경상북도\", \"경북\"],\n",
    "    \"경남\": [\"경상남도\", \"경남\"],\n",
    "    \"제주\": [\"제주특별자치도\", \"제주도\", \"제주\"],\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 유틸\n",
    "# =========================================\n",
    "def read_csv_auto(path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    CSV 인코딩 자동 판별 로더.\n",
    "    - utf-8-sig 먼저 시도 → 실패 시 cp949 재시도.\n",
    "    - 둘 다 실패하면 None 반환.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    for enc in (\"utf-8-sig\", \"cp949\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, dtype=str, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# 카테고리 분리: \",\", \"/\", \"·\", \".\"\n",
    "CAT_SPLIT_RE = re.compile(r\"\\s*(?:,|/|·|\\.)\\s*\")\n",
    "\n",
    "def split_category(s: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    원본 업종/분류 문자열을 구분자(, / · .)로 분리하고\n",
    "    공백 제거 + 중복 제거(순서 보존)하여 리스트로 반환.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    raw = [t.strip() for t in CAT_SPLIT_RE.split(s) if t and t.strip()]\n",
    "    seen, out = set(), []\n",
    "    for t in raw:\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def to_json_list(lst: List[str]) -> str:\n",
    "    \"\"\"파이썬 리스트 → JSON 문자열('[\"a\",\"b\"]')로 직렬화.\"\"\"\n",
    "    return json.dumps(lst, ensure_ascii=False)\n",
    "\n",
    "def normalize_region(region: Optional[str], address: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    지역 문자열을 REGION_VARIANTS 기준으로 표준 키로 통일.\n",
    "    - region 값이 비었으면 address에서 추론.\n",
    "    - 매칭 실패 시 빈 문자열(\"\") 반환.\n",
    "    \"\"\"\n",
    "    cand = (region or \"\").strip()\n",
    "    addr = (address or \"\").strip()\n",
    "\n",
    "    def _match(text: str) -> Optional[str]:\n",
    "        if not text:\n",
    "            return None\n",
    "        for key, vars_ in REGION_VARIANTS.items():\n",
    "            for v in vars_:\n",
    "                if v in text:\n",
    "                    return key\n",
    "        return None\n",
    "\n",
    "    reg = _match(cand) or _match(addr)\n",
    "    return reg or \"\"\n",
    "\n",
    "def drop_blank_name(df: pd.DataFrame, name_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    이름 컬럼이 공백/결측인 행 제거.\n",
    "    - 소스별 name_col은 SOURCES[name_col]에서 지정.\n",
    "    \"\"\"\n",
    "    mask = df[name_col].astype(str).str.strip() != \"\"\n",
    "    return df.loc[mask].copy()\n",
    "\n",
    "# =========================================\n",
    "# 소스별 표준화 규칙\n",
    "#  - 각 소스의 파일명과, 이름/주소/날짜/업종(분류) 컬럼명을 정의\n",
    "#  - ref_date는 date_cols 우선순위대로 첫 값 채택\n",
    "# =========================================\n",
    "SOURCES = {\n",
    "    \"포털DB\": {\n",
    "        \"file\": \"포털DB_merged.csv\",\n",
    "        \"name_col\": \"상호명\",\n",
    "        \"date_cols\": [\"업데이트날짜\", \"업데이트\"],\n",
    "        \"addr_col\": \"주소\",\n",
    "        \"raw_cat_col\": \"업종\",\n",
    "    },\n",
    "    \"기업DB\": {\n",
    "        \"file\": \"기업DB_merged.csv\",\n",
    "        \"name_col\": \"업체명\",\n",
    "        \"date_cols\": [\"등록일\"],\n",
    "        \"addr_col\": \"주소\",\n",
    "        \"raw_cat_col\": \"업종\",\n",
    "    },\n",
    "    \"인허가DB\": {\n",
    "        \"file\": \"인허가DB_merged.csv\",\n",
    "        \"name_col\": \"업체명\",\n",
    "        \"date_cols\": [\"업데이트날짜\", \"업데이트\"],\n",
    "        \"addr_col\": \"주소\",\n",
    "        \"raw_cat_col\": \"업종\",\n",
    "    },\n",
    "    \"법인DB\": {\n",
    "        \"file\": \"법인DB_merged.csv\",\n",
    "        \"name_col\": \"업체명\",\n",
    "        \"date_cols\": [\"업데이트날짜\", \"업데이트\"],\n",
    "        \"addr_col\": \"주소\",\n",
    "        \"raw_cat_col\": \"업종\",\n",
    "    },\n",
    "    \"마켓DB\": {\n",
    "        \"file\": \"마켓DB_merged.csv\",\n",
    "        \"name_col\": \"업체명\",\n",
    "        \"date_cols\": [\"등록일\"],\n",
    "        \"addr_col\": \"주소\",\n",
    "        \"raw_cat_col\": \"분류\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 스테이징 표준 스키마(컬럼 순서 고정)\n",
    "STD_COLS = [\n",
    "    \"region\", \"source_db\", \"merchant_code\",\n",
    "    \"name\", \"address\", \"ref_date\", \"raw_category\", \"category\"\n",
    "]\n",
    "\n",
    "# =========================================\n",
    "# 메인 변환\n",
    "# =========================================\n",
    "def build_staging() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    *_merged.csv 들을 읽어 표준 스키마로 변환 후 하나의 DataFrame으로 병합.\n",
    "    처리 내용:\n",
    "      - region 정규화(없으면 주소에서 추론)\n",
    "      - category 분리(구분자 기반) → JSON 문자열로 저장\n",
    "      - 이름 공백/결측 행 제거\n",
    "      - ref_date는 소스별 후보 중 첫 값 채택\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for source_db, cfg in SOURCES.items():\n",
    "        path = MERGED_DIR / cfg[\"file\"]\n",
    "        df = read_csv_auto(path)\n",
    "        if df is None:\n",
    "            print(f\"[INFO] skip: 파일 없음 → {path.name}\")\n",
    "            continue\n",
    "\n",
    "        # 결측은 일단 공백으로 채움(후속 가공 일관성)\n",
    "        df = df.fillna(\"\")\n",
    "\n",
    "        # 소스별 컬럼명 매핑\n",
    "        name_col  = cfg[\"name_col\"]\n",
    "        addr_col  = cfg[\"addr_col\"]\n",
    "        raw_col   = cfg[\"raw_cat_col\"]\n",
    "        date_cols = cfg[\"date_cols\"]\n",
    "\n",
    "        # 표준 스키마 그리기\n",
    "        out = pd.DataFrame()\n",
    "\n",
    "        # region: 존재하면 사용, 없으면 주소에서 추론(특히 기업DB 대비)\n",
    "        has_region = \"region\" in df.columns\n",
    "        out[\"region\"] = [\n",
    "            normalize_region(\n",
    "                df.loc[i, \"region\"] if has_region else \"\",\n",
    "                df.loc[i, addr_col] if addr_col in df.columns else \"\"\n",
    "            )\n",
    "            for i in df.index\n",
    "        ]\n",
    "\n",
    "        # source_db: 고정 값\n",
    "        out[\"source_db\"] = source_db\n",
    "\n",
    "        # merchant_code: 원본 통합 단계에서 이미 부여된 코드 사용(없으면 공백)\n",
    "        out[\"merchant_code\"] = df[\"merchant_code\"] if \"merchant_code\" in df.columns else \"\"\n",
    "\n",
    "        # name / address\n",
    "        out[\"name\"] = df[name_col] if name_col in df.columns else \"\"\n",
    "        out[\"address\"] = df[addr_col] if addr_col in df.columns else \"\"\n",
    "\n",
    "        # ref_date: date_cols 우선순위대로 첫 값 채움\n",
    "        ref_date_series = pd.Series([\"\"] * len(df))\n",
    "        for dc in date_cols:\n",
    "            if dc in df.columns:\n",
    "                empty_mask = ref_date_series.eq(\"\")\n",
    "                ref_date_series.loc[empty_mask] = df[dc].astype(str).fillna(\"\")\n",
    "        out[\"ref_date\"] = ref_date_series\n",
    "\n",
    "        # 원본 업종/분류 → raw_category\n",
    "        out[\"raw_category\"] = df[raw_col] if raw_col in df.columns else \"\"\n",
    "\n",
    "        # category: 구분자 분리 리스트를 JSON 문자열로 직렬화\n",
    "        out[\"category\"] = out[\"raw_category\"].map(lambda s: to_json_list(split_category(s)))\n",
    "\n",
    "        # 이름 공백/결측 행 제거\n",
    "        out = drop_blank_name(out, \"name\")\n",
    "\n",
    "        # 표준 컬럼 순서 재배치 + 결측 공백\n",
    "        out = out.reindex(columns=STD_COLS).fillna(\"\")\n",
    "        frames.append(out)\n",
    "\n",
    "        print(f\"[OK] {source_db}: {len(out):,}행\")\n",
    "\n",
    "    # 소스에서 단 한 건도 못 읽었을 때\n",
    "    if not frames:\n",
    "        print(\"[WARN] 수집된 데이터가 없습니다.\")\n",
    "        return pd.DataFrame(columns=STD_COLS)\n",
    "\n",
    "    # 최종 병합 + 결측 공백\n",
    "    staging = pd.concat(frames, axis=0, ignore_index=True, sort=False).fillna(\"\")\n",
    "    return staging\n",
    "\n",
    "# =========================================\n",
    "# 실행 진입점\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 출력 폴더 보장\n",
    "    MERGED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 스테이징 DF 생성\n",
    "    staging = build_staging()\n",
    "\n",
    "    # 저장 (UTF-8 + CP949)\n",
    "    staging.to_csv(OUT_PATH_UTF8, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # CP949 호환본(손실 문자는 무시하고 저장)\n",
    "    staging_cp = staging.applymap(\n",
    "        lambda x: x.encode(\"cp949\", errors=\"ignore\").decode(\"cp949\") if isinstance(x, str)\n",
    "        else (\"\" if pd.isna(x) else x)\n",
    "    )\n",
    "    staging_cp.to_csv(OUT_PATH_CP949, index=False, encoding=\"cp949\")\n",
    "\n",
    "    print(f\"\\n✅ 저장 완료 → {OUT_PATH_UTF8.name} / {OUT_PATH_CP949.name} (총 {len(staging):,}행)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
