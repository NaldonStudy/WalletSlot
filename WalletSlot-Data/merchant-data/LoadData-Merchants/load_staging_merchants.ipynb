{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2151b853-7ee9-4f64-9f31-366d7fcdeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 행: 13471842  / 중복 코드가 있는 행: 0\n",
      "Series([], Name: count, dtype: int64)\n",
      "Empty DataFrame\n",
      "Columns: [region, source_db, merchant_code, name, address, ref_date, raw_category, category]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# merchant_code 중복 있는지 확인\n",
    "import pandas as pd\n",
    "\n",
    "path = \"./_merged_csv/staging_all.utf8.csv\"  # 경로 맞게 수정\n",
    "for enc in (\"utf-8-sig\", \"cp949\"):\n",
    "    try:\n",
    "        df = pd.read_csv(path, dtype=str, encoding=enc)\n",
    "        break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "dups = df[\"merchant_code\"].duplicated(keep=False)       # 중복 전체 표시\n",
    "print(\"총 행:\", len(df), \" / 중복 코드가 있는 행:\", dups.sum())\n",
    "\n",
    "# 중복 코드 목록 + 건수 (상위 20개 보기)\n",
    "vc = df[\"merchant_code\"].value_counts()\n",
    "print(vc[vc > 1].head(20))\n",
    "\n",
    "# 중복된 행 샘플 10개 보기\n",
    "print(df[dups].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2104b87f-e614-43f9-a026-faae1f7d019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 총 13,471,842행을 ~5MB 기준으로 분할 저장: _merged_csv\\staging_all_split\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def split_csv_by_size(infile: str, target_mb: int = 5, encoding: str = \"utf-8-sig\"):\n",
    "    \"\"\"\n",
    "    대용량 CSV를 용량 기준으로 여러 개의 파일로 분할합니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    infile : str\n",
    "        분할할 원본 CSV 경로(상대/절대경로 모두 가능).\n",
    "        예: \"./_merged_csv/staging_all.utf8.csv\"\n",
    "    target_mb : int, default=5\n",
    "        각 분할 파일의 목표 용량(MB). 정확히 딱 맞추는 건 아니고,\n",
    "        파일 포인터 크기 기준으로 '이상'이 되는 시점에 새 파일을 엽니다.\n",
    "    encoding : str, default=\"utf-8-sig\"\n",
    "        입력/출력 모두 동일 인코딩으로 사용합니다. (윈도우 호환을 위해 기본값 BOM 포함)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    <원본과 같은 폴더>/staging_all_split/ 아래에\n",
    "    \"staging_all.part001.csv\", \"staging_all.part002.csv\", ... 형태로 생성됩니다.\n",
    "    각 파트에는 항상 헤더를 포함하며, 줄바꿈은 CRLF로 고정합니다.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - 분할 기준은 파일 핸들의 현재 크기(f_out.tell())를 사용합니다.\n",
    "      한 줄이 써진 뒤 threshold를 넘으면 즉시 다음 파일로 넘어갑니다.\n",
    "    - 헤더는 모든 파트의 첫 줄에 다시 기록됩니다.\n",
    "    - 출력 인코딩 또한 encoding 파라미터를 따릅니다.\n",
    "    \"\"\"\n",
    "\n",
    "    in_path = Path(infile)\n",
    "    if not in_path.exists():\n",
    "        raise FileNotFoundError(f\"입력 파일이 없습니다: {in_path}\")\n",
    "\n",
    "    # 출력 폴더: 원본 CSV와 동일 폴더 아래 'staging_all_split'\n",
    "    out_dir = in_path.parent / \"staging_all_split\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    target_bytes = target_mb * 1024 * 1024  # MB → byte\n",
    "    part = 1                 # part 번호(001부터 시작)\n",
    "    total_rows = 0           # 전체 데이터(헤더 제외) 누적 카운트\n",
    "    writer = None            # 현재 csv.writer\n",
    "    f_out = None             # 현재 출력 파일 핸들\n",
    "    header = None            # 헤더 행 보관\n",
    "\n",
    "    def new_part():\n",
    "        \"\"\"새로운 파트 파일을 열고 헤더를 다시 기록하는 헬퍼 함수.\"\"\"\n",
    "        nonlocal part, writer, f_out, header\n",
    "        # 이전 파일이 열려 있으면 닫기\n",
    "        if f_out:\n",
    "            f_out.close()\n",
    "        out_path = out_dir / f\"staging_all.part{part:03d}.csv\"\n",
    "        # newline=\"\"로 열고, writer에서 lineterminator를 CRLF로 강제 설정\n",
    "        f_out = out_path.open(\"w\", encoding=encoding, newline=\"\")\n",
    "        writer = csv.writer(f_out, lineterminator=\"\\r\\n\")\n",
    "        # 첫 줄: 헤더 재작성\n",
    "        writer.writerow(header)\n",
    "        part += 1\n",
    "\n",
    "    # 입력 파일 오픈\n",
    "    with in_path.open(\"r\", encoding=encoding, newline=\"\") as f_in:\n",
    "        reader = csv.reader(f_in)\n",
    "        # 첫 줄은 헤더\n",
    "        try:\n",
    "            header = next(reader)\n",
    "        except StopIteration:\n",
    "            # 빈 CSV라면 바로 종료\n",
    "            print(\"[INFO] 빈 CSV 입니다. 작업 없음.\")\n",
    "            return\n",
    "\n",
    "        # 첫 번째 파트 생성\n",
    "        new_part()\n",
    "\n",
    "        # 데이터 행을 순차 기록\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "            total_rows += 1\n",
    "\n",
    "            # 현재 파일 크기가 기준을 넘으면 새 파트 시작\n",
    "            if f_out.tell() >= target_bytes:\n",
    "                new_part()\n",
    "\n",
    "    # 마지막 파일 핸들 정리\n",
    "    if f_out:\n",
    "        f_out.close()\n",
    "\n",
    "    print(f\"[OK] 총 {total_rows:,}행을 ~{target_mb}MB 기준으로 분할 저장: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 상대경로 예시: 현재 스크립트 위치 기준\n",
    "    # 필요시 target_mb를 더 작게/크게 조절하세요.\n",
    "    split_csv_by_size(\"./_merged_csv/staging_all.utf8.csv\", target_mb=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4528bc-68e8-42d5-bf16-29471ca5e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 처리할 신규 파일 없음 (.DONE 모두 존재)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "staging_all.partXXX.csv 분할 파일들을 '빠르게' 병렬로 MySQL에 적재합니다.\n",
    "\n",
    "핵심 최적화\n",
    "- executemany 대신 'INSERT ... VALUES (..),(..),(..)' 멀티-VALUES로 전송 → 왕복 횟수↓\n",
    "- 파일당 1회 커밋(배치 플러시만) → COMMIT 오버헤드↓\n",
    "- 적절한 스레드 수(기본 3) → InnoDB 경합 완화\n",
    "\n",
    "규약\n",
    "- CSV 헤더: region, source_db, merchant_code, name, address, ref_date, raw_category, category\n",
    "- DB 컬럼 순서: id(AI), merchant_code, name, category(JSON), raw_category, region, address, source_date, source_db, ingested_at\n",
    "- INSERT(IGNORE 아님): merchant_code UNIQUE 위반 시 에러 → 파일 .FAIL 생성\n",
    "- 성공 시 .DONE 생성(재실행 시 스킵)\n",
    "\"\"\"\n",
    "\n",
    "import os, csv, json, time, glob, traceback\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "\n",
    "# ===== SSH =====\n",
    "SSH_HOST = \"j13b108.p.ssafy.io\"\n",
    "SSH_PORT = 22\n",
    "SSH_USER = \"ubuntu\"\n",
    "SSH_KEY  = r\"C:\\Users\\SSAFY\\Desktop\\j13B108T.pem\"\n",
    "\n",
    "# ===== MySQL (터널 기준 내부 주소/포트) =====\n",
    "DB_HOST_INSIDE_SSH = \"127.0.0.1\"\n",
    "DB_PORT_INSIDE_SSH = 3306\n",
    "DB_USER = \"admin\"\n",
    "DB_PASS = \"dnxlachl\"\n",
    "DB_NAME = \"walletslotdb\"\n",
    "\n",
    "# ===== CSV 분할 파일 폴더(상대경로) =====\n",
    "SPLIT_DIR = Path(\"./_merged_csv/staging_all_split\")\n",
    "PATTERN   = \"staging_all.part*.csv\"\n",
    "\n",
    "# ===== 배치/병렬 설정 =====\n",
    "BATCH_SIZE  = 10000     # 10k~20k 권장 (max_allowed_packet 상황에 맞춰 조절)\n",
    "MAX_WORKERS = 3         # 2~3 추천(과다 동시성은 오히려 느려짐)\n",
    "\n",
    "# ===== 테이블 생성 =====\n",
    "CREATE_TABLE_SQL = \"\"\"\n",
    "create table if not exists staging_merchants (\n",
    "  id            bigint unsigned auto_increment primary key,\n",
    "  merchant_code varchar(32) not null,\n",
    "  name          varchar(255) null,\n",
    "  category      json         null,\n",
    "  raw_category  text         null,\n",
    "  region        varchar(32)  null,\n",
    "  address       varchar(255) null,\n",
    "  source_date   varchar(32)  null,\n",
    "  source_db     varchar(16)  not null,\n",
    "  ingested_at   timestamp not null default current_timestamp,\n",
    "  unique key uk_merchant_code (merchant_code),\n",
    "  key idx_source_db (source_db),\n",
    "  key idx_region (region),\n",
    "  key idx_ingested_at (ingested_at)\n",
    ") engine=innodb default charset=utf8mb4;\n",
    "\"\"\"\n",
    "\n",
    "# 멀티-VALUES INSERT 프리픽스/플레이스홀더\n",
    "INSERT_PREFIX = \"\"\"\n",
    "insert into staging_merchants\n",
    "(merchant_code, name, category, raw_category, region, address, source_date, source_db, ingested_at)\n",
    "values\n",
    "\"\"\"\n",
    "ROW_PH = \"(%s,%s,CAST(%s AS JSON),%s,%s,%s,%s,%s,CURRENT_TIMESTAMP)\"\n",
    "\n",
    "def sanitize_category(s: str) -> str:\n",
    "    \"\"\"category 문자열을 JSON 배열로 보정.\"\"\"\n",
    "    if not s or s.strip() == \"\":\n",
    "        return \"[]\"\n",
    "    try:\n",
    "        j = json.loads(s)\n",
    "        return json.dumps(j, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return \"[]\"\n",
    "\n",
    "def iter_rows_from_csv(path: str):\n",
    "    \"\"\"\n",
    "    CSV 헤더는 고정:\n",
    "    region, source_db, merchant_code, name, address, ref_date, raw_category, category\n",
    "    INSERT 순서:\n",
    "    (merchant_code, name, category, raw_category, region, address, source_date, source_db)\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            mc = (row.get(\"merchant_code\") or \"\").strip()\n",
    "            if not mc:\n",
    "                continue\n",
    "            name        = (row.get(\"name\") or \"\").strip() or None\n",
    "            category    = sanitize_category(row.get(\"category\"))\n",
    "            raw_cat     = (row.get(\"raw_category\") or \"\").strip() or None\n",
    "            region      = (row.get(\"region\") or \"\").strip() or None\n",
    "            address     = (row.get(\"address\") or \"\").strip() or None\n",
    "            source_date = (row.get(\"ref_date\") or \"\").strip() or None\n",
    "            source_db   = (row.get(\"source_db\") or \"\").strip() or None\n",
    "            yield (mc, name, category, raw_cat, region, address, source_date, source_db)\n",
    "\n",
    "def mark(path: str, suffix: str, content: str = \"\"):\n",
    "    \"\"\"path+'.SUFFIX' 마커 파일 생성\"\"\"\n",
    "    m = f\"{path}.{suffix}\"\n",
    "    with open(m, \"w\", encoding=\"utf-8\") as f:\n",
    "        if content:\n",
    "            f.write(content)\n",
    "\n",
    "def has_mark(path: str, suffix: str) -> bool:\n",
    "    return os.path.exists(f\"{path}.{suffix}\")\n",
    "\n",
    "def exec_batch_multi_values(cur, batch):\n",
    "    \"\"\"\n",
    "    batch: [(mc, name, category_json, raw, region, addr, date, src), ...]\n",
    "    멀티-VALUES 한 번에 전송\n",
    "    \"\"\"\n",
    "    sql = INSERT_PREFIX + \",\".join([ROW_PH] * len(batch))\n",
    "    args = []\n",
    "    for rec in batch:\n",
    "        args.extend(rec)\n",
    "    cur.execute(sql, args)\n",
    "\n",
    "def load_one_file(local_port: int, csv_path: str):\n",
    "    \"\"\"\n",
    "    한 파일을 멀티-VALUES 배치로 INSERT.\n",
    "    반환: (파일명, 읽은행, 삽입행, 초, 상태)\n",
    "    \"\"\"\n",
    "    if has_mark(csv_path, \"DONE\"):\n",
    "        return (os.path.basename(csv_path), 0, 0, 0.0, \"SKIP_DONE\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    conn = None\n",
    "    n_rows = n_ins = 0\n",
    "    status = \"OK\"\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=local_port,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            database=DB_NAME,\n",
    "            charset=\"utf8mb4\",\n",
    "            autocommit=False,   # 파일당 1회 커밋\n",
    "            read_timeout=600,\n",
    "            write_timeout=600,\n",
    "        )\n",
    "        with conn.cursor() as cur:\n",
    "            # 타임아웃만 늘림 (unique_checks=1 유지: 중복 차단 보장)\n",
    "            cur.execute(\"set session net_write_timeout=600, net_read_timeout=600\")\n",
    "            cur.execute(\"set session wait_timeout=28800, interactive_timeout=28800\")\n",
    "            cur.execute(\"set session foreign_key_checks=0\")\n",
    "\n",
    "            batch = []\n",
    "            for rec in iter_rows_from_csv(csv_path):\n",
    "                batch.append(rec)\n",
    "                n_rows += 1\n",
    "                if len(batch) >= BATCH_SIZE:\n",
    "                    exec_batch_multi_values(cur, batch)\n",
    "                    n_ins += cur.rowcount\n",
    "                    batch.clear()\n",
    "\n",
    "            if batch:\n",
    "                exec_batch_multi_values(cur, batch)\n",
    "                n_ins += cur.rowcount\n",
    "                batch.clear()\n",
    "\n",
    "            # 파일 단위 1회 커밋\n",
    "            conn.commit()\n",
    "\n",
    "        mark(csv_path, \"DONE\")\n",
    "        if has_mark(csv_path, \"FAIL\"):\n",
    "            try: os.remove(f\"{csv_path}.FAIL\")\n",
    "            except Exception: pass\n",
    "\n",
    "    except Exception as e:\n",
    "        status = f\"ERR: {type(e).__name__}: {e}\"\n",
    "        mark(csv_path, \"FAIL\", content=traceback.format_exc())\n",
    "        if conn:\n",
    "            try: conn.rollback()\n",
    "            except Exception: pass\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "    return (os.path.basename(csv_path), n_rows, n_ins, time.time() - t0, status)\n",
    "\n",
    "def main():\n",
    "    files = sorted(glob.glob(str(SPLIT_DIR / PATTERN)))\n",
    "    if not files:\n",
    "        print(f\"[INFO] 분할 파일 없음: {SPLIT_DIR}\\\\{PATTERN}\")\n",
    "        return\n",
    "\n",
    "    files = [fp for fp in files if not has_mark(fp, \"DONE\")]\n",
    "    if not files:\n",
    "        print(\"[INFO] 처리할 신규 파일 없음 (.DONE 모두 존재)\")\n",
    "        return\n",
    "\n",
    "    with SSHTunnelForwarder(\n",
    "        (SSH_HOST, SSH_PORT),\n",
    "        ssh_username=SSH_USER,\n",
    "        ssh_pkey=SSH_KEY,\n",
    "        remote_bind_address=(DB_HOST_INSIDE_SSH, DB_PORT_INSIDE_SSH),\n",
    "    ) as tunnel:\n",
    "        tunnel.start()\n",
    "        local_port = tunnel.local_bind_port\n",
    "        print(f\"[SSH] 127.0.0.1:{local_port} -> {DB_HOST_INSIDE_SSH}:{DB_PORT_INSIDE_SSH}\")\n",
    "\n",
    "        # 테이블 보장\n",
    "        conn = pymysql.connect(\n",
    "            host=\"127.0.0.1\",\n",
    "            port=local_port,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS,\n",
    "            database=DB_NAME,\n",
    "            charset=\"utf8mb4\",\n",
    "            autocommit=True,\n",
    "        )\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(CREATE_TABLE_SQL)\n",
    "        conn.close()\n",
    "\n",
    "        total_rows = total_ins = 0\n",
    "        t_all = time.time()\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futs = [ex.submit(load_one_file, local_port, fp) for fp in files]\n",
    "            for fut in as_completed(futs):\n",
    "                fn, n_rows, n_ins, secs, status = fut.result()\n",
    "                total_rows += n_rows\n",
    "                total_ins  += n_ins\n",
    "                print(f\"[DONE] {Path(fn).name:>24s} | 읽음 {n_rows:>8,} / 삽입 {n_ins:>8,} | {secs:5.1f}s | {status}\")\n",
    "\n",
    "        print(f\"\\n✅ 전체 합계: 읽음 {total_rows:,} / 삽입 {total_ins:,} | 총 {time.time()-t_all:.1f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e33c7d-6062-4768-a292-43b4793f4e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
